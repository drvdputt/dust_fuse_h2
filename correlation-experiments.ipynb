{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "feaa795e-e26f-4705-ab03-358f0f9d652a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Experiment when multiplying two variables with common parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6d6dd2-62a1-41dd-af7b-360c16e52fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c47ba68-3bf0-40e3-a480-5663c1db9cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from covariance import plot_scatter_density\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "rng = default_rng()\n",
    "N = 1000\n",
    "x = rng.normal(16, 3, N)\n",
    "y = rng.normal(8, 2, N)\n",
    "a = rng.normal(4, 1, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f0a712-38db-49f9-9c12-15beb82d5141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def compare_xy_xya(x, y, a):\n",
    "    fig, axs = plt.subplots(1, 2)\n",
    "    axs[0].scatter(x,y)\n",
    "    axs[1].scatter(x*a, y*a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37bc640-d54b-4bdc-996c-4da87eb6bcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_xy_xya(x, y, a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f545a3ff-c237-4052-96ab-09703fd7a745",
   "metadata": {},
   "source": [
    "A correlation is clearly induced by multiplying with a common factor (even if there is no uncertainty on that factor). But it actually tells you something: x/a and y/a are correlated because of changes in a. If x/a and y/a were interesting physical quantities by themselves, then this is proof that the underlying quantities x and y are independent, but the observations are correlated due to a.\n",
    "\n",
    "## Inherently uncorrelated: constant nh/av + noise\n",
    "\n",
    "now let's generate some data where NH/AV is constant + noise, and try to recreate something that looks like our (av, nh, a1000) data\n",
    "\n",
    "Here, I demonstrate what happens if a constant NH/AV and A1000/NH2 are assumed. A1000 and AV are derived from NH and NH2, with noise added. Then NH/AV and A1000/NH2 are calculated (even if assumed constant, the noise will introduce differences). The result is that the mock observations of NH/AV and A1000/NH2 are uncorrelated!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0328050d-364a-4d30-ae16-7a078304b3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assumptions\n",
    "\n",
    "# constant gas to dust\n",
    "nh_av = 2\n",
    "\n",
    "# constant h2 to a1000\n",
    "a1000_nh2 = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e708546c-95ab-4fa9-9931-b8c139b5a0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mock(plot=False):\n",
    "    N = 100\n",
    "    a = rng.normal(4, 1, N)\n",
    "    c = 2\n",
    "    nh_noise = 0.5\n",
    "    nh = nh_av * a + rng.normal(0, nh_noise, len(a))\n",
    "    fh2 = rng.uniform(0, 1, N)\n",
    "    nh2 = nh * fh2 / 2\n",
    "    # let's also model a1000 with a constant a1000/NH2 ratio of ~ 11\n",
    "    a1000_noise = 3\n",
    "    a1000 = nh2 * a1000_nh2 + rng.normal(0, a1000_noise, len(nh2))\n",
    "\n",
    "    if plot:\n",
    "        fig, axs = plt.subplots(1, 3)\n",
    "        axs[0].scatter(a, nh)\n",
    "        axs[1].scatter(a1000, nh)\n",
    "        axs[2].scatter(a1000, nh2)\n",
    "        \n",
    "        fig, axs = plt.subplots()\n",
    "        axs.scatter(a1000/a, nh/a)\n",
    "        \n",
    "    r = np.corrcoef(a1000/a, nh/a)[0,1]\n",
    "    return r "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27cf27e-0313-4b33-a5ca-ffa70ea6b174",
   "metadata": {},
   "outputs": [],
   "source": [
    "mock(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1a6abf-8cd5-4851-a6ce-5748472b0819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = 10000\n",
    "# rhos = np.zeros(N)\n",
    "# rho0s = np.zeros(N)\n",
    "# xbase = a1000/a\n",
    "# ybase = nh/a\n",
    "# for i in range(N):\n",
    "#     rhos[i] = mock()\n",
    "#     xc = xbase.copy()\n",
    "#     yc = ybase.copy()\n",
    "#     np.random.shuffle(xc)\n",
    "#     rho0s[i] = np.corrcoef(xc, yc)[0,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0eedbf3-f8d4-4768-8194-ef7d0ebd1d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(rhos, bins=50)\n",
    "# plt.hist(rho0s, bins=50)\n",
    "# plt.axvline(np.median(rhos), color='k')\n",
    "# print(np.median(rhos))\n",
    "# print(np.std(rho0s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f59231-8fef-491e-bd1f-a644e144b8e8",
   "metadata": {},
   "source": [
    "# Monte Carlo Pearson test\n",
    "## Model\n",
    "\n",
    "Physical model: x and y are uncorrelated\n",
    "\n",
    "Noise model: x' and y' strongly correlated\n",
    "\n",
    "Question: does my Pearson coefficient method work, i.e. does it give a low correlation significance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b2dd44-422f-4c3a-b70a-027855cd1e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def physical_sample():\n",
    "    N = 75\n",
    "    mx = 0\n",
    "    sx = 1\n",
    "    my = 0\n",
    "    sy = 1\n",
    "    mr = (mx, my)\n",
    "    covr = [[sx**2, 0],[0, sy**2]]\n",
    "    rs = rng.multivariate_normal(mr, covr, size=N)\n",
    "    return rs\n",
    "\n",
    "def measured_sample(sigma, rho, rs = None):\n",
    "    if rs is None:\n",
    "        rs_real = physical_sample()\n",
    "    else:\n",
    "        rs_real = rs\n",
    "    N = len(rs_real)\n",
    "    x_unc = sigma\n",
    "    y_unc = sigma\n",
    "    xy_unc_cov = rho * x_unc * y_unc\n",
    "    # use same covariance matrix for every point, for simplicity\n",
    "    cov_unc = np.array([[x_unc**2, xy_unc_cov], [xy_unc_cov, y_unc**2]])\n",
    "    unc_noise = rng.multivariate_normal((0,0), cov_unc, size=N)\n",
    "    rs_measured = rs_real + unc_noise\n",
    "    # create array of copies of the covariance matrix, to use in my tools\n",
    "    covs = np.tile(cov_unc, (N, 1, 1))\n",
    "    return rs_measured, covs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0c04bb-4438-48e1-8d68-f546f2a7630c",
   "metadata": {},
   "source": [
    "## Induced correlation: single cases\n",
    "\n",
    "We also check what my monte carlo pearson method says, and if it results in the same conclusions we can see visually here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17cb478-6a17-4522-815d-e8e996c2fdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from covariance import plot_scatter_density\n",
    "from pearson import pearson_mc_nocov\n",
    "from plot_fuse_results import plot_rho_box\n",
    "\n",
    "def visual_example(sigma, rho, method='nocov'):\n",
    "    rs_p = physical_sample()\n",
    "    rs, covs = measured_sample(sigma, rho, rs_p)\n",
    "    fig, ax = plt.subplots(1, 2, sharex=True, sharey=True)\n",
    "    ax[0].scatter(rs_p[:,0], rs_p[:,1], marker='+', color='k')\n",
    "    plot_scatter_density(ax[1], rs[:, 0], rs[:, 1], covs)\n",
    "    plot_rho_box(ax[1], rs[:, 0], rs[:, 1], covs, method=method)\n",
    "    ax[0].set_xlim(-5, 5)\n",
    "    ax[1].set_ylim(-5, 5)\n",
    "    ax[0].set_xlim(-5, 5)\n",
    "    ax[1].set_ylim(-5, 5)\n",
    "    fig.savefig(f\"induced_correlation_example_s{sigma}_r{rho}.pdf\")\n",
    "    #coeff, std = pearson_mc_nocov(rs[:, 0], rs[:, 1], covs, hist_ax=ax[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3842f694-6012-4f2a-84ea-292f2d6c5c2d",
   "metadata": {},
   "source": [
    "### Large error + correlation = correlation induced\n",
    "\n",
    "Now that I have implemented my new covariance method, we can compare the two!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7d7cb1-e7db-4125-83bd-f68e6fb73b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_example(1, 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca60436b-e7b8-4eaf-902f-48634fe95588",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_example(2, 0.99, method='cov approx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fa874b-2649-4b21-a0e8-77daa4c82f79",
   "metadata": {},
   "source": [
    "### Small error + correlation = no correlation induced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f0cf8b-fb8e-4b6b-90ab-57bc2d2c6a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_example(.2, 0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b964d04-ac48-405e-b033-34ec452ae0b2",
   "metadata": {},
   "source": [
    "### Large error + small correlation = no induced?\n",
    "\n",
    "With big errors, outliers might induce some correlation on rare occasions. Let's work with histograms then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86595a7f-7378-4096-981d-28fcabdcf5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_example(1, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6367aec0-791e-4d36-a6ee-e0503ef8c3b4",
   "metadata": {},
   "source": [
    "## Induced correlation: ensemble -- shift of r-histogram\n",
    "\n",
    "This first function compares the $\\rho$ distributions between physical set and the set with measurement errors that might have induced correlations.\n",
    "\n",
    "The physics and measurements are mocked many times to build histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475b1d3a-7848-4c0b-879f-ec7ec2422aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_rhodist(sigma, rho):\n",
    "    M = 1000\n",
    "    rhos_physics = np.zeros(M)\n",
    "    rhos_measured = np.zeros(M)\n",
    "    for i in range(M):\n",
    "        rs = physical_sample()\n",
    "        rs_m, covs = measured_sample(sigma=sigma, rho=rho, rs=rs)\n",
    "        rhos_physics[i] = np.corrcoef(rs[:, 0], rs[:, 1])[0,1]\n",
    "        rhos_measured[i] = np.corrcoef(rs_m[:, 0], rs_m[:, 1])[0,1]\n",
    "    plt.hist(rhos_physics,  bins=33, label='physics')\n",
    "    plt.hist(rhos_measured, bins=33, label='measured')\n",
    "    plt.xlabel('r')\n",
    "    plt.ylabel('num')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return rhos_physics, rhos_measured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3657db7-c681-47be-9beb-612ab65d8928",
   "metadata": {},
   "outputs": [],
   "source": [
    "rhos_physics, rhos_measured = compare_rhodist(1, 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db79bd8f-9594-4cf3-ba65-7d47ce2a12ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "rhos_physics, rhos_measured = compare_rhodist(1, 0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b313ee-148c-43b2-8893-ba16a213b7fb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1aa41b99-3a3c-4747-906c-0b36b0ac51b8",
   "metadata": {},
   "source": [
    "## Constructing a null hypothesis sample with induced correlation\n",
    "\n",
    "Side note: there is such a thing as \"correlation attenuation\". Generally, with uncorrelated noise, the observed correlation coefficient will always be smaller than the physical correlation. This is already discussed in the original paper by Spearman (1907). Some equations for corrected correlation coefficients can be found in https://www.nature.com/articles/s41598-019-57247-4#Equ9, but those usually need the error to be the same for all data points.\n",
    "\n",
    "Therefore, taking the median of the rho(measured) samples is not really the way to go.\n",
    "\n",
    "Let's focus specifically on the case of A1000/AV and NH/AV. NH and A1000 are already correlated because of column density. Both also correlate with AV for the same reason. By dividing by AV, we are removing the column density factor, and looking for additional correlations. The correlation because of the uncertainty on AV makes this hard. If the uncertainty on AV is small, then there should be no problem. \n",
    "\n",
    "We want to create a sample of A1000/AV and NH/AV where the two are uncorrelated. This implies that both hover around some value (which could depend on AV / NH / A1000, but that effect is supressed because of the ratio). If the two ratios were uncorrelated, then we should be able to sample them independently. The only problem is knowing which shifts to use due to the measurements, and the covariance matrices to do this depend on the underlying AV/NH/A1000 values.\n",
    "\n",
    "In my common denominator equation (see covariance.py), I need AV and sigmaAV, but not A1000 and NH! So what I could do, is sample a random underlying AV, which should be fine if AV is assumed to not correlated with A1000/AV and NH/AV. Not necessarily true, but probably good enough for this exercise, since NH vs AV looks linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61742185-0957-4f17-9dde-643ff52bdb10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the real data\n",
    "from pearson import RandomCovMock, CommonDenominatorMock, pearson_mock_test\n",
    "from rescale import RescaledData\n",
    "from get_data import get_merged_table, get_xs_ys_covs, get_param_and_unc\n",
    "data = get_merged_table()\n",
    "xs, ys, covs = get_xs_ys_covs(data, \"A1000_AV\", \"NH_AV\")\n",
    "ignore_high = ys < 4e21\n",
    "xs = xs[ignore_high]\n",
    "ys = ys[ignore_high]\n",
    "covs = covs[ignore_high]\n",
    "rd = RescaledData(xs, ys, covs, xfactor_yfactor=(1, 1e-21))\n",
    "xs1, ys1, covs1 = rd.xs, rd.ys, rd.covs\n",
    "# make the covs bigger to see if the method is working properly\n",
    "#covs1[:, 0, 1] = 0.95 * np.sqrt(covs1[:,0,0] * covs1[:,1,1])\n",
    "#covs1[:, 1, 0] = covs1[:, 0, 1]\n",
    "\n",
    "avs, avs_unc = get_param_and_unc('AV', data)\n",
    "avs = avs[ignore_high]\n",
    "avs_unc = avs_unc[ignore_high]\n",
    "\n",
    "denom_mock = CommonDenominatorMock(xs1, ys1, covs1, avs, avs_unc)\n",
    "random_mock = RandomCovMock(xs1, ys1, covs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d54c972-2756-48b0-8e2b-b2aa2e177853",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter_density(plt.gca(), xs1, ys1, covs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00089ef-91c8-4028-89e4-d871cdfe0ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try the common denom method\n",
    "pearson_mock_test(denom_mock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1d9739-8c4f-46ec-b809-939586c84005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try the common denom method\n",
    "pearson_mock_test(random_mock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f9cc7a-bf76-4748-9b99-7e4f92df5e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter_density(plt.gca(), *random_mock.mock())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c1590e-b65c-4efa-977f-fcb1ccfebf0e",
   "metadata": {},
   "source": [
    "### Same with A2175"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea58bbb-a5b3-43c4-b8f3-b6e0263bebc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xs, ys, covs = get_xs_ys_covs(data, \"A2175_AV\", \"NH_AV\")\n",
    "ignore_high = ys < 4e21\n",
    "xs = xs[ignore_high]\n",
    "ys = ys[ignore_high]\n",
    "covs = covs[ignore_high]\n",
    "rd = RescaledData(xs, ys, covs, xfactor_yfactor=(1, 1e-21))\n",
    "xs2, ys2, covs2 = rd.xs, rd.ys, rd.covs\n",
    "\n",
    "# make the covs bigger to see if the method is working properly\n",
    "# covs2[:, 0, 1] = 0.95 * np.sqrt(covs2[:,0,0] * covs2[:,1,1])\n",
    "# covs2[:, 1, 0] = covs2[:, 0, 1]\n",
    "# covs2 *= 4\n",
    "# cov matrix needs to be adjust correctly along with avs_unc. \n",
    "# Only straightforward if the original data are used.\n",
    "# just scaling the cov matrix should work better\n",
    "\n",
    "# if I don't adjust avs_unc here, then the common demoninator \n",
    "# method will not work correctly for this made up scenario with inflated covs.\n",
    "factor = 4\n",
    "avs_unc_boost = factor * avs_unc\n",
    "covs2 *= factor * factor\n",
    "\n",
    "denom_mock2 = CommonDenominatorMock(xs2, ys2, covs2, avs, avs_unc_boost)\n",
    "random_mock2 = RandomCovMock(xs2, ys2, covs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d774a529-c429-4f46-9319-88a4083f76a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter_density(plt.gca(), xs2, ys2, covs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98050bd5-41d6-46c8-ae71-f5418c01c7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try the common denom method\n",
    "pearson_mock_test(denom_mock2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30987839-541f-44a7-9117-12e0fb90d1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try the common denom method\n",
    "pearson_mock_test(random_mock2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b47310d-a5cc-4f25-aab3-51ecc4e4a43d",
   "metadata": {},
   "source": [
    "# Investigating problem: wiggle removing rho\n",
    "\n",
    "occurs for the A1000/AV vs NH/AV plots\n",
    "\n",
    "I think I found it: forgot to rescale. Should have listened to the warning that multivariate_normal was giving me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd19b9e-ac7b-4b53-bfcd-98d6a37d57fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys, covs = get_xs_ys_covs(data, \"A1000_AV\", \"NH_AV\")\n",
    "ignore_high = ys < 4e21\n",
    "xs = xs[ignore_high]\n",
    "ys = ys[ignore_high]\n",
    "covs = covs[ignore_high]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e569a4bb-91a9-412a-82bb-e0e63da9cc37",
   "metadata": {},
   "source": [
    "# Look at some plots with strong correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b99d2ee-2ed0-47ae-a26e-40f3596ed322",
   "metadata": {},
   "source": [
    "## with fake data\n",
    "\n",
    "this fake data is inherently uncorrelated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cd5599-5584-4cd7-8a3d-e8b16bc259c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_m, covs = measured_sample(sigma=1, rho=0.99)\n",
    "xs, ys = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8fcffb-449c-43a3-9dbf-14c82d6bf553",
   "metadata": {},
   "source": [
    "## comparing extinction ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44476b0d-2572-4834-9ac1-9262402ea47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys, covs = get_xs_ys_covs(data, \"A2175_NH\", \"AV_NH\")\n",
    "fig, ax = plt.subplots()\n",
    "plot_scatter_density(ax, xs, ys, covs)\n",
    "plot_rho_box(ax, xs, ys, covs, method='nocov')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30c54ec-b57e-4287-97f8-db8732c47509",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys, covs = get_xs_ys_covs(data, \"A2175_NH\", \"AV_NH\")\n",
    "fig, ax = plt.subplots()\n",
    "plot_scatter_density(ax, xs, ys, covs)\n",
    "plot_rho_box(ax, xs, ys, covs, method='cov approx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bea079-5a27-46d9-a199-7a0cfda659dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys, covs = get_xs_ys_covs(data, \"fh2\", \"NH_AV\")\n",
    "fig, ax = plt.subplots()\n",
    "plot_scatter_density(ax, xs, ys, covs)\n",
    "plot_rho_box(ax, xs, ys, covs, method='cov approx', optional_plot_fname='strongcorr.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1d3f76-dfe8-4818-b594-2d62dd7ae1ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
